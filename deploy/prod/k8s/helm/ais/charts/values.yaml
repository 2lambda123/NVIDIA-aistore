# Default values for ais (proxy and target).
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  aisnode:
    repository: quay.io/nvidia/aisnode
    tag: stable
  kubectl:
    repository: quay.io/nvidia/ais-kubectl
    tag: stable
  pullPolicy: IfNotPresent
  pullSecretNames:
    - ais-pull-secret

#
# "Common" ais.json config values - actually not genuinely common and applicable to
# all of electable/nonelectable/target, but since those templates have duplicated
# sections we may as well avoid also duplicating values.
#
# Don't add anything here that does not contribute to ais.json; and only add
# parametrized elements to _ais_common.json - no literal values there.
#
ais_config_common:
  auth:
    secret:  aBitLongSecretKey
    enabled: false
    creddir: ""
  cksum:
    type: xxhash
    validate_cold_get: true
    validate_warm_get: false
    validate_obj_move: false
    enable_read_range: false
  cloud_provider: ""
  compression:
    block_size: 262144
    checksum: false
  confdir: /etc/ais
  disk:
    disk_util_low_wm:  20
    disk_util_high_wm: 80
    disk_util_max_wm:  95
    iostat_time_long:  2s
    iostat_time_short: 100ms
  distributed_sort:
    compression:           never
    duplicated_records:    ignore
    missing_shards:        ignore
    ekm_malformed_line:    abort
    ekm_missing_key:       abort
    default_max_mem_usage: 80%
    dsorter_mem_threshold: 100GB
    call_timeout:          10m
  downloader:
    timeout: 1h
  ec:
    enabled:       false
    objsize_limit: 262144
    data_slices:   2
    parity_slices: 2
    compression:   never
  fshc:
    enabled:       true
    test_files:    4
    error_limit:   2
  keepalivetracker:
    proxy:
      interval: 10s
      name:     heartbeat
      factor:   3
    target:
      interval: 10s
      name:     heartbeat
      factor:   3
    retry_factor: 5
    timeout_factor: 3
  log:
    dir:       /var/log/ais
    level:     3
    max_size:  4194304
    max_total: 67108864
  lru:
    lowwm:             75
    highwm:            90
    out_of_space:      95
    dont_evict_time:   120m
    capacity_upd_time: 10m
    enabled:           true
  mirror:
    copies:            2
    burst_buffer:      512
    util_thresh:       0
    optimize_put:      false
    enabled:           false
  periodic:
    stats_time:      10s
    retry_sync_time: 2s
  rebalance:
    enabled:         true
    compression:     never
    dest_retry_time: 2m
    quiescent:       20s
    multiplier:      4
  timeout:
    default_timeout:      10s
    default_long_timeout: 30m
    list_timeout:         10m
    max_keepalive:        4s
    proxy_ping:           100ms
    cplane_operation:     2s
    send_file_time:       5m
    startup_time:         1m
  versioning:
    enabled:           true
    validate_warm_get: false

ais_k8s:
  cluster_cidr:           ""
  clusterIP_gw_label:     shared
  container_capabilities:
    # Needed for debug if you wish to run delve within a pod (or look into kubesquash etc)
    #- SYS_PTRACE
  sysctls:
    somaxconn: 100000

proxy:
  name: proxy        # A component label for selector
  gw_label: "shared"  # shared means in proxy clusterIP service
  config:
    proxy:
      non_electable: false
      discovery_url: ""
    test_fspaths:
      count:    0
      instance: 0
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51080
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    131072
      http:
        write_buffer_size: 0
        read_buffer_size:  0
        use_https:        false
        chunked_transfer: true
  service:
    type: ClusterIP
    port: 51080
  nodeSelector:
    key: nvidia.com/ais-proxy
  resources: {}
  # Apply the below node label on any node (just 1), the proxy runs on that node will become a primary at launch
  initialPrimaryProxyNodeLabel:
    name:  "nvidia.com/ais-initial-primary-proxy"
  # The path of the environment file to be passed into the ais container
  envMountPath:
    hostPath: /tmp/
    podPath:  /var/ais_env
  etcMountPath:
    hostPath: /etc/ais/proxy
    # No pod path as it should be the same as config.dir

ne_proxy:
  name: "ne_proxy"    # A component label for selector
  gw_label: "shared"   # shared means in proxy clusterIP service
  config:
    proxy:
      non_electable: true
      discovery_url: ""
    test_fspaths:
      count:    0
      instance: 0
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51080
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    131072
      http:
        write_buffer_size: 0
        read_buffer_size:  0
        use_https:        false
        chunked_transfer: true
  service:
    type: ClusterIP
    port: 51080      # must match that of proxy since they go behind same clusterIP service
  nodeSelector:
    key:   nvidia.com/ais-proxy
  resources: {}
  # The path of the environment file to be passed into the ais container
  envMountPath:
    hostPath: /tmp/
    podPath:  /var/ais_env
  etcMountPath:
    hostPath: /etc/ais/proxy

target:
  name : "target"   # A component label for selector
  config:
    proxy:
      non_electable: false
      discovery_url: ""
    test_fspaths:
      count:    0
      instance: 0
    nodiskio:
      enabled:    false
      dryobjsize: "8M"
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51081
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    0
      http:
        write_buffer_size: 65536
        read_buffer_size:  65536
        use_https:         false
        chunked_transfer:  true
  service:
    port:     51081
    hostport: 0
  nodeSelector:
    key:   nvidia.com/ais-target
  resources: {}
  etcMountPath:
    hostPath: /etc/ais/target
    # No pod path as it should be the same as config.dir
  envMountPath:
    hostPath: /tmp/
    podPath:  /var/ais_env
  mountPaths:
    - /ais/sda
    - /ais/sdb
    - /ais/sdc
    - /ais/sdd
    - /ais/sde
    - /ais/sdf
    - /ais/sdg
    - /ais/sdh
    - /ais/sdi
    - /ais/sdj

#
# Ingress to the proxy/gateway service
#
ingress:
  gateway:
    externalIP:
    port:       51080
    targetPort: 51080
  grafana:
    externalIP:
    port:       3000
    targetPort: 3000


#
# Note that out target/proxy/ne_proxy DaemonSets use any resource values from their
# respective values sections.
#
resources: {}

#
# Storage nodes can/should be tainted to keep non-AIS loads off, so we must
# tolerate those taints. The same tolerations are added to graphite and grafana
# subcharts (because their PV will often be satisfied from a storage node, and
# we don't want them running on any GPU nodes).
#
tolerations:
- key:      "dedicated"
  operator: "Equal"
  value:    "ais"
  effect:   "PreferNoSchedule"

# Make sure the DFC target only deploy to the node that are marked with a label that signify a hi-perf
# storage
# target-node-key-name : target-node
affinity: {}
#affinity:
#  requiredDuringSchedulingIgnoredDuringExecution:
#    nodeSelectorTerms:
#    - matchExpressions:
#        - key: beta.kubernetes.io/instance-type
#          operator: In
#          values:
#            - d1.8xlarge

#
# If you have an existing graphite installation then set the builtin_monitoring
# tag to false and supply the host (or IP) and port for graphite in
# map external_monitoring.
#
# If builtin_monitoring is true and you don't want this chart to install
# Prometheus then set the prometheus tag to false.
#
tags:
  builtin_monitoring: true
  prometheus:         true

#
# Alternatively, leave the builtin-monitoring tag true (the default) and
# we'll use subchart dependencies to deploy graphite and grafana within the k8s
# cluster.
#
# If data persistence is enabled for Graphite and Grafana then local storage
# must already have been assigned on the indicated node and path combinations
# below - we don't create the underlying storage here, we're just creating a PV
# from existing local storage to satisfy the PVC made from graphite and grafana.
#
# XXX TODO:
#
#   - would be nice to add some standard dashboards; do this via another sidecar
#

#
# Key paths here that match those of https://github.com/kiwigrid/helm-charts/tree/master/charts/graphite
# will over-ride default values in the graphite dependency. Local additions are all within the ais map.
#
graphite:
  persistence:
    enabled:       true
    existingClaim: graphite-pvc
  ais:
    pv:
      capacity: "250Gi"
      path:     ""
      node:     ""
  tolerations:
  - key:      "dedicated"
    operator: "Equal"
    value:    "ais"
    effect:   "PreferNoSchedule"

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/grafana
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
grafana:
  persistence:
    enabled:       true
    existingClaim: grafana-pvc
  ais:
    pv:
      capacity: "250Gi"
      path:     ""
      node:     ""
  service:
    type: NodePort
  sidecar:
    datasources:
      enabled: true
      label:   ais_grafana_datasource
    dashboards:
      enabled: false
      label:   ais_grafana_dashboard
  tolerations:
  - key:      "dedicated"
    operator: "Equal"
    value:    "ais"
    effect:   "PreferNoSchedule"

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/prometheus
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
# XXX TODO enable persistence
#
prometheus:
  alertmanager:
    persistentVolume:
      enabled: false
    tolerations:
    - key:      "dedicated"
      operator: "Equal"
      value:    "ais"
      effect:   "PreferNoSchedule"
  server:
    persistentVolume:
      enabled: false
    tolerations:
    - key:      "dedicated"
      operator: "Equal"
      value:    "ais"
      effect:   "PreferNoSchedule"
  kubeStateMetrics:
    tolerations:
    - key:      "dedicated"
      operator: "Equal"
      value:    "ais"
      effect:   "PreferNoSchedule"
  nodeExporter:
    tolerations:
    - key:      "dedicated"
      operator: "Equal"
      value:    "ais"
      effect:   "PreferNoSchedule"
  pushgateway:
    tolerations:
    - key:      "dedicated"
      operator: "Equal"
      value:    "ais"
      effect:   "PreferNoSchedule"


#
# Used only if builtin_monitoring is over-ridden to false. No Grafana or Prometheus here - we
# just arrange to send AIS stats to Graphite, and the external provider is responsible for
# node metrics, visualization etc.
#
external_monitoring:
  graphite_host: somehost
  graphite_port: 2003

pre_install_hook:
  etcMountPath:
    hostPath: /etc/ais
